{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-10T16:17:50.152883Z",
     "iopub.status.busy": "2025-04-10T16:17:50.152114Z",
     "iopub.status.idle": "2025-04-10T16:21:14.790658Z",
     "shell.execute_reply": "2025-04-10T16:21:14.789559Z",
     "shell.execute_reply.started": "2025-04-10T16:17:50.152846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y huggingface_hub\n",
    "!pip install huggingface_hub==0.15.1\n",
    "!pip install pydantic-settings==2.0.3\n",
    "!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\n",
    "bitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.15\n",
    "!pip install pypdf llama_index\n",
    "!pip uninstall pymongo -y\n",
    "!pip install \"pymongo[srv]==3.13.0\"\n",
    "!pip install pyngrok\n",
    "\n",
    "import dns\n",
    "!pip install scipy==1.10.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T16:21:14.792499Z",
     "iopub.status.busy": "2025-04-10T16:21:14.792188Z",
     "iopub.status.idle": "2025-04-10T16:21:20.754858Z",
     "shell.execute_reply": "2025-04-10T16:21:20.754186Z",
     "shell.execute_reply.started": "2025-04-10T16:21:14.792472Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T16:21:20.755991Z",
     "iopub.status.busy": "2025-04-10T16:21:20.755716Z",
     "iopub.status.idle": "2025-04-10T16:21:29.419607Z",
     "shell.execute_reply": "2025-04-10T16:21:29.419033Z",
     "shell.execute_reply.started": "2025-04-10T16:21:20.755968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "    from torch import cuda, bfloat16\n",
    "    import torch\n",
    "    import transformers\n",
    "    from transformers import AutoTokenizer\n",
    "    from time import time\n",
    "    import chromadb\n",
    "    from chromadb.config import Settings\n",
    "    from langchain.llms import HuggingFacePipeline\n",
    "    from langchain.document_loaders import TextLoader\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.vectorstores import Chroma\n",
    "    \n",
    "    import json\n",
    "    from pymongo import MongoClient\n",
    "    import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T16:21:29.421901Z",
     "iopub.status.busy": "2025-04-10T16:21:29.421340Z",
     "iopub.status.idle": "2025-04-10T16:21:29.634218Z",
     "shell.execute_reply": "2025-04-10T16:21:29.633654Z",
     "shell.execute_reply.started": "2025-04-10T16:21:29.421880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Model details\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"  # Example for LLaMA 2 7B Chat Model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Quantization Configuration\n",
    "# for gpu\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T16:21:29.635141Z",
     "iopub.status.busy": "2025-04-10T16:21:29.634906Z",
     "iopub.status.idle": "2025-04-10T16:21:36.069399Z",
     "shell.execute_reply": "2025-04-10T16:21:36.068439Z",
     "shell.execute_reply.started": "2025-04-10T16:21:29.635123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install triton==2.1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T16:21:36.070682Z",
     "iopub.status.busy": "2025-04-10T16:21:36.070421Z",
     "iopub.status.idle": "2025-04-10T16:23:34.937709Z",
     "shell.execute_reply": "2025-04-10T16:23:34.936805Z",
     "shell.execute_reply.started": "2025-04-10T16:21:36.070657Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "time_1 = time()\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    ")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "time_2 = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T16:23:34.939401Z",
     "iopub.status.busy": "2025-04-10T16:23:34.938679Z",
     "iopub.status.idle": "2025-04-10T16:23:38.440321Z",
     "shell.execute_reply": "2025-04-10T16:23:38.439448Z",
     "shell.execute_reply.started": "2025-04-10T16:23:34.939371Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "time_1 = time()\n",
    "query_pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",)\n",
    "time_2 = time()\n",
    "print(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T16:23:38.441712Z",
     "iopub.status.busy": "2025-04-10T16:23:38.441225Z",
     "iopub.status.idle": "2025-04-10T16:23:38.446270Z",
     "shell.execute_reply": "2025-04-10T16:23:38.445507Z",
     "shell.execute_reply.started": "2025-04-10T16:23:38.441675Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_model(tokenizer, pipeline, prompt_to_test):\n",
    "    \"\"\"\n",
    "    Perform a query\n",
    "    print the result\n",
    "    Args:\n",
    "        tokenizer: the tokenizer\n",
    "        pipeline: the pipeline\n",
    "        prompt_to_test: the prompt\n",
    "    Returns\n",
    "        None\n",
    "    \"\"\"\n",
    "    # adapted from https://huggingface.co/blog/llama2#using-transformers\n",
    "    time_1 = time()\n",
    "    sequences = pipeline(\n",
    "        prompt_to_test,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,)\n",
    "    time_2 = time()\n",
    "    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T16:23:38.447495Z",
     "iopub.status.busy": "2025-04-10T16:23:38.446949Z",
     "iopub.status.idle": "2025-04-10T16:23:51.525322Z",
     "shell.execute_reply": "2025-04-10T16:23:51.524483Z",
     "shell.execute_reply.started": "2025-04-10T16:23:38.447477Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_model(tokenizer,\n",
    "           query_pipeline,\n",
    "           \"Please explain what are Newton's Law of Motion. Keep it in 100 words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T16:23:51.528137Z",
     "iopub.status.busy": "2025-04-10T16:23:51.527893Z",
     "iopub.status.idle": "2025-04-10T16:23:57.261235Z",
     "shell.execute_reply": "2025-04-10T16:23:57.260468Z",
     "shell.execute_reply.started": "2025-04-10T16:23:51.528119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
    "# checking again that everything is working fine\n",
    "llm(prompt=\"Please explain what are Newton's Law of Motion. Keep it in 100 words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T16:23:57.262513Z",
     "iopub.status.busy": "2025-04-10T16:23:57.262251Z",
     "iopub.status.idle": "2025-04-10T16:24:13.138412Z",
     "shell.execute_reply": "2025-04-10T16:24:13.137775Z",
     "shell.execute_reply.started": "2025-04-10T16:23:57.262488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-large-en\"  # ✅ Better retrieval model\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, \n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "print(\"Embedding model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T16:24:13.139506Z",
     "iopub.status.busy": "2025-04-10T16:24:13.139222Z",
     "iopub.status.idle": "2025-04-10T16:24:29.128680Z",
     "shell.execute_reply": "2025-04-10T16:24:29.127697Z",
     "shell.execute_reply.started": "2025-04-10T16:24:13.139481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install cohere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T17:06:25.370104Z",
     "iopub.status.busy": "2025-04-10T17:06:25.369598Z",
     "iopub.status.idle": "2025-04-10T17:15:20.968189Z",
     "shell.execute_reply": "2025-04-10T17:15:20.967344Z",
     "shell.execute_reply.started": "2025-04-10T17:06:25.370080Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cohere  \n",
    "from flask import Flask, request, jsonify\n",
    "from pyngrok import ngrok, conf\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "\n",
    "# 🔹 Set your NGROK_AUTH_TOKEN here\n",
    "NGROK_AUTH_TOKEN = \"\"  # ⬅️ Replace with your actual token\n",
    "conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
    "\n",
    "# Initialize Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# MongoDB Connection\n",
    "client = MongoClient(\"\")\n",
    "db = client[\"\"]\n",
    "collection = db[\"\"]\n",
    "\n",
    "# ChromaDB Setup\n",
    "chroma_path = \"./chroma_db\"\n",
    "os.makedirs(chroma_path, exist_ok=True)  \n",
    "\n",
    "# Initialize persistent ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=chroma_path)\n",
    "co = cohere.Client(\"\")  # ✅ Use your API key\n",
    "\n",
    "\n",
    "\n",
    "def strip_extra_conversation(response_text):\n",
    "    \"\"\"\n",
    "    Preserves the first Bot or 🔹 Bot response while stripping any subsequent content.\n",
    "    \"\"\"\n",
    "    # Identify the markers indicating Bot responses\n",
    "    strip_markers = [\"\\nBot:\", \"\\n🔹 Bot:\"]\n",
    "    \n",
    "    # Find the earliest occurrence of either \"Bot:\" or \"🔹 Bot:\"\n",
    "    first_marker_index = None\n",
    "    for marker in strip_markers:\n",
    "        if marker in response_text:\n",
    "            marker_index = response_text.find(marker)\n",
    "            if first_marker_index is None or marker_index < first_marker_index:\n",
    "                first_marker_index = marker_index\n",
    "\n",
    "    # If a Bot marker exists, keep everything before it (including the first Bot response)\n",
    "    if first_marker_index is not None:\n",
    "        response_text = response_text[:first_marker_index + len(\"🔹 Bot:\")].strip()\n",
    "\n",
    "    return response_text\n",
    "\n",
    "\n",
    "# 🛠 Create separate Chroma collections per key\n",
    "def get_chroma_collection(key):\n",
    "    return Chroma(\n",
    "        persist_directory=f\"./chroma_db/{key}\",\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "\n",
    "# Function to fetch data from MongoDB\n",
    "def fetch_from_mongodb(key):\n",
    "    \"\"\"Fetches data from MongoDB using the key.\"\"\"\n",
    "    data = collection.find_one({\"key\": key})\n",
    "    return data[\"text_data\"] if data else None\n",
    "\n",
    "\n",
    "def setup_chromadb_for_website(website_name, documents, embeddings):\n",
    "    \"\"\"\n",
    "    Initialize and persist ChromaDB for a specific website.\n",
    "    \"\"\"\n",
    "    directory = f\"chroma_db/{website_name.replace(' ', '_')}\"\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=directory\n",
    "    )\n",
    "    return vectordb\n",
    "\n",
    "def upload_to_chromadb(key, text_content, embeddings):\n",
    "    \"\"\"\n",
    "    Uploads data to ChromaDB after retrieving from MongoDB.\n",
    "    \"\"\"\n",
    "    print(f\"📤 Uploading data to ChromaDB for key: {key}...\")\n",
    "\n",
    "    # Load and split documents\n",
    "    temp_file_path = f\"./{key}.txt\"\n",
    "    with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text_content)\n",
    "\n",
    "    loader = TextLoader(temp_file_path, encoding=\"utf8\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Setup ChromaDB for the website\n",
    "    vectordb = setup_chromadb_for_website(key, all_splits, embeddings)\n",
    "    os.remove(temp_file_path)  # Cleanup temp file\n",
    "\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "def get_chroma_collection(key):\n",
    "    \"\"\"Returns an existing ChromaDB collection or None if missing.\"\"\"\n",
    "    try:\n",
    "        collection_path = f\"./chroma_db/{key}\"\n",
    "        if not os.path.exists(collection_path):\n",
    "            print(f\"🚨 ChromaDB collection not found for key: {key}\")\n",
    "            return None  # No collection exists yet\n",
    "\n",
    "        return Chroma(\n",
    "            persist_directory=collection_path,\n",
    "            embedding_function=embeddings  # Ensure embeddings are initialized\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error accessing ChromaDB for key {key}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "@app.route(\"/query\", methods=[\"POST\"])\n",
    "def process_query():\n",
    "    \"\"\"Handles a user query with conversational context.\"\"\"\n",
    "    data = request.get_json()\n",
    "    query = data.get(\"query\")\n",
    "    key = data.get(\"key\")\n",
    "    user_id = data.get(\"user_id\")  # Ensure client sends a unique user ID\n",
    "\n",
    "    if not query or not key or not user_id:\n",
    "        return jsonify({\"error\": \"Missing query, key, or user ID\"}), 400\n",
    "\n",
    "    if query.lower() == \"resethistory\":\n",
    "        # Delete all conversation history for the given user_id\n",
    "        collection2 = db[\"history\"]\n",
    "        result = collection2.delete_many({\"user_id\": user_id})\n",
    "\n",
    "        return jsonify({\n",
    "            \"response\": f\"Conversation history has been reset for user_id: {user_id}.\",\n",
    "            \"deleted_count\": result.deleted_count\n",
    "        })\n",
    "    \n",
    "    # 🔹 Retrieve previous conversation context\n",
    "    collection2 = db[\"history\"]\n",
    "    \n",
    "    conversation = collection2.find_one({\"user_id\": user_id, \"session_id\": key})\n",
    "    history = conversation[\"messages\"] if conversation else []\n",
    "\n",
    "    # Format conversation history for better context\n",
    "    formatted_history = \"\\n\".join([f\"{msg['role']}: {msg['text']}\" for msg in history])\n",
    "    context_query = f\"You are a friendly, professional, and knowledgeable customer support agent at the company retrieved from document. Your goal is to provide accurate, concise, and helpful responses. Answer the user's query based on the available data. Avoid repeating fallback statements unless you are certain the data is unavailable. Remain polite and approachable.\\n{formatted_history}\\nUser: {query}\\nBot:\"\n",
    "\n",
    "    # 🔍 Step 1: Try retrieving from ChromaDB\n",
    "    vectordb = get_chroma_collection(key)\n",
    "    if vectordb:  \n",
    "        retriever = vectordb.as_retriever(search_kwargs={\"k\": 7})\n",
    "\n",
    " \n",
    "        qa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        docs = qa.retriever.get_relevant_documents(context_query)\n",
    "        print(docs)\n",
    "\n",
    "\n",
    "        # 🔹 Step 3: Apply Reranking\n",
    "        rerank_input = [doc.page_content for doc in docs]  # ✅ Extract text for reranking\n",
    "        reranked_docs = co.rerank(model=\"rerank-english-v2.0\", query=query, documents=rerank_input, top_n=10)\n",
    "        \n",
    "        # Keep only the top reranked docs\n",
    "        docs = [docs[item.index] for item in reranked_docs.results] \n",
    "\n",
    "        \n",
    "        if docs:\n",
    "            response = qa.run(context_query)\n",
    "            print(response)\n",
    "\n",
    "            response = strip_extra_conversation(response)\n",
    "\n",
    "            # Store new conversation data\n",
    "            collection2.update_one(\n",
    "                {\"user_id\": user_id, \"session_id\": key},\n",
    "                {\"$push\": {\"messages\": {\"role\": \"user\", \"text\": query}}},\n",
    "                upsert=True\n",
    "            )\n",
    "            collection2.update_one(\n",
    "                {\"user_id\": user_id, \"session_id\": key},\n",
    "                {\"$push\": {\"messages\": {\"role\": \"bot\", \"text\": response}}},\n",
    "                upsert=True\n",
    "            )\n",
    "\n",
    "            return jsonify({\"response\": response})\n",
    "\n",
    "    # 🔄 Step 2: Fallback to MongoDB if ChromaDB has no data\n",
    "    mongo_data = fetch_from_mongodb(key)\n",
    "    if mongo_data:\n",
    "        upload_to_chromadb(key, mongo_data, embeddings)  # Store in ChromaDB\n",
    "        vectordb = get_chroma_collection(key)  \n",
    "\n",
    "        if vectordb:\n",
    "            retriever = vectordb.as_retriever(search_kwargs={\"k\": 7})\n",
    "\n",
    "            qa = RetrievalQA.from_chain_type(\n",
    "                llm=llm,\n",
    "                chain_type=\"stuff\",\n",
    "                retriever=retriever,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            docs = qa.retriever.get_relevant_documents(context_query)\n",
    "            print(docs)\n",
    "            rerank_input = [doc.page_content for doc in docs]  # ✅ Extract text for reranking\n",
    "            reranked_docs = co.rerank(model=\"rerank-english-v2.0\", query=query, documents=rerank_input, top_n=10)\n",
    "        \n",
    "        # Keep only the top reranked docs\n",
    "            docs = [docs[item.index] for item in reranked_docs.results]  \n",
    "\n",
    "            \n",
    "            if docs:\n",
    "                response = qa.run(context_query)\n",
    "                print(response)\n",
    "                response = strip_extra_conversation(response)\n",
    "\n",
    "                # Store conversation data\n",
    "                collection2.update_one(\n",
    "                    {\"user_id\": user_id, \"session_id\": key},\n",
    "                    {\"$push\": {\"messages\": {\"role\": \"user\", \"text\": query}}},\n",
    "                    upsert=True\n",
    "                )\n",
    "                collection2.update_one(\n",
    "                    {\"user_id\": user_id, \"session_id\": key},\n",
    "                    {\"$push\": {\"messages\": {\"role\": \"bot\", \"text\": response}}},\n",
    "                    upsert=True\n",
    "                )\n",
    "\n",
    "                return jsonify({\"response\": response})\n",
    "\n",
    "    return jsonify({\"response\": \"No relevant data found.\"})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 🔹 Start an ngrok tunnel with authentication\n",
    "    public_url = ngrok.connect(5000)\n",
    "    print(f\"🔗 Ngrok tunnel established: {public_url}\")\n",
    "\n",
    "    # Run the Flask app\n",
    "    app.run(host=\"0.0.0.0\", port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
